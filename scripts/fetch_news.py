#!/usr/bin/env python3
"""
NASDAQ 100 ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° â€” ë„¤ì´ë²„ ê²€ìƒ‰ API
ë§¤ì¼ GitHub Actionsì—ì„œ ì‹¤í–‰, ê²°ê³¼ë¥¼ data/news.jsonìœ¼ë¡œ ëˆ„ì  ì €ì¥

í•µì‹¬:
- 90ì¼(3ê°œì›”) ëˆ„ì  ë°©ì‹
- ê²½ì œ/ê¸ˆìœµ ë‰´ìŠ¤ë§Œ í•„í„° (ì—”í„°/ë¬¸í™” ì œì™¸)
- ì¤‘ë³µ ê¸°ì‚¬ ì œê±° (URL + ì œëª© ìœ ì‚¬ë„)
- ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ ìš°íšŒ ì°¨ë‹¨
"""

import os, json, time, urllib.request, urllib.parse, re
from datetime import datetime, timezone, timedelta
from email.utils import parsedate_to_datetime

CLIENT_ID = os.environ["NAVER_CLIENT_ID"]
CLIENT_SECRET = os.environ["NAVER_CLIENT_SECRET"]

RETENTION_DAYS = 90

# â•â•â• í—ˆìš© ì–¸ë¡ ì‚¬ ë„ë©”ì¸ (ê²½ì œ/ê¸ˆìœµ ì „ë¬¸ì§€ë§Œ) â•â•â•
ALLOWED_DOMAINS = [
    "mk.co.kr",           # ë§¤ì¼ê²½ì œ
    "heraldcorp.com",      # í—¤ëŸ´ë“œê²½ì œ
    "herald.co.kr",        # í—¤ëŸ´ë“œê²½ì œ (êµ¬ë„ë©”ì¸)
    "fnnews.com",          # íŒŒì´ë‚¸ì…œë‰´ìŠ¤
    "mt.co.kr",            # ë¨¸ë‹ˆíˆ¬ë°ì´
    "moneytoday.co.kr",    # ë¨¸ë‹ˆíˆ¬ë°ì´ (êµ¬ë„ë©”ì¸)
    "bizwatch.co.kr",      # ë¹„ì¦ˆì›Œì¹˜
    "asiae.co.kr",         # ì•„ì‹œì•„ê²½ì œ
    "edaily.co.kr",        # ì´ë°ì¼ë¦¬
    "biz.chosun.com",      # ì¡°ì„ ë¹„ì¦ˆ
    "hankyung.com",        # í•œêµ­ê²½ì œ
    "joseilbo.com",        # ì¡°ì„¸ì¼ë³´
    "sedaily.com",         # ì„œìš¸ê²½ì œ
]

# â•â•â• ì°¨ë‹¨ ë„ë©”ì¸ â•â•â•
BLOCKED_DOMAINS = [
    "starnewskorea.com",
    "star.mt.co.kr",
    "stoo.com",
    "osen.mt.co.kr",
    "osen.co.kr",
    "entertain.",
    "sports.",
    "star.",
    "n.news.naver.com",
    "news.naver.com",
]

# â•â•â• ë¹„ê²½ì œ ë‰´ìŠ¤ ì œì™¸ í‚¤ì›Œë“œ â•â•â•
EXCLUDE_KEYWORDS = [
    "ë“œë¼ë§ˆ", "ì˜í™”", "ì¶œì—°", "ë°°ìš°", "ê°ë…", "ì‹œì²­ë¥ ", "ì˜ˆëŠ¥",
    "ì•„ì´ëŒ", "ì»´ë°±", "ë°ë·”", "ë®¤ì§ë¹„ë””ì˜¤", "ìŒì›",
    "íŒ¬ë¯¸íŒ…", "ì½˜ì„œíŠ¸", "ìºìŠ¤íŒ…", "ì´¬ì˜", "ê°œë´‰", "í¥í–‰",
    "ë°•ìŠ¤ì˜¤í”¼ìŠ¤", "OST", "ì›¹íˆ°", "ì›¹ì†Œì„¤", "ì• ë‹ˆë©”ì´ì…˜",
    "ì—´ì• ", "ê²°í˜¼", "ì´í˜¼", "ìŠ¤ìº”ë“¤",
]

# â•â•â• ê²½ì œ í‚¤ì›Œë“œ (ì œì™¸ í‚¤ì›Œë“œì™€ í•¨ê»˜ ìˆìœ¼ë©´ í—ˆìš©) â•â•â•
FINANCE_KEYWORDS = [
    "ì£¼ê°€", "ì‹œì´", "ë§¤ì¶œ", "ì‹¤ì ", "íˆ¬ì", "ì¦ì‹œ", "ë‚˜ìŠ¤ë‹¥",
    "ë°˜ë„ì²´", "AI", "ì¸ê³µì§€ëŠ¥", "í´ë¼ìš°ë“œ", "ë°ì´í„°ì„¼í„°",
    "ë§¤ìˆ˜", "ë§¤ë„", "ëª©í‘œê°€", "ì „ë§", "ë¶„ê¸°",
    "ì˜ì—…ì´ìµ", "ìˆœì´ìµ", "ë°°ë‹¹", "IPO", "ìƒì¥",
    "ê³µê¸‰ë§", "ìˆ˜ì¶œ", "ê´€ì„¸", "ê·œì œ", "ì¸ìˆ˜", "í•©ë³‘",
    "CEO", "ê²½ì˜", "ì „ëµ", "ë§¤ê°", "íŒŒíŠ¸ë„ˆì‹­",
    "ë‹¬ëŸ¬", "í™˜ìœ¨", "ê¸ˆë¦¬", "ì—°ì¤€",
]


def is_allowed_source(url):
    if not url:
        return False
    url_lower = url.lower()
    for blocked in BLOCKED_DOMAINS:
        if blocked in url_lower:
            return False
    return any(domain in url_lower for domain in ALLOWED_DOMAINS)


def is_financial_news(title, desc):
    combined = title + " " + desc
    exclude_count = sum(1 for kw in EXCLUDE_KEYWORDS if kw in combined)
    if exclude_count >= 2:
        return False
    if exclude_count >= 1:
        has_finance = any(kw in combined for kw in FINANCE_KEYWORDS)
        if not has_finance:
            return False
    return True


def normalize_title(title):
    """ì œëª© ì •ê·œí™” â€” ì¤‘ë³µ ë¹„êµìš©"""
    return re.sub(r"[^\wê°€-í£]", "", title).lower()


def is_duplicate(new_title, new_url, existing_articles):
    """URL + ì œëª© ìœ ì‚¬ë„ë¡œ ì¤‘ë³µ ì²´í¬"""
    # URL ì •ê·œí™” (íŒŒë¼ë¯¸í„° ì œê±°)
    def clean_url(u):
        return u.split("?")[0].split("#")[0].rstrip("/") if u else ""

    clean_new = clean_url(new_url)
    norm_new = normalize_title(new_title)

    for art in existing_articles:
        # URL ì¤‘ë³µ
        if clean_new and clean_new == clean_url(art.get("url", "")):
            return True

        # ì œëª© ìœ ì‚¬ë„ â€” ì •ê·œí™”ëœ ì œëª©ì˜ ì• 20ìê°€ ê°™ìœ¼ë©´ ì¤‘ë³µ
        norm_ex = normalize_title(art.get("title", ""))
        if len(norm_new) >= 15 and len(norm_ex) >= 15:
            if norm_new[:20] == norm_ex[:20]:
                return True

    return False


# â•â•â• í‹°ì»¤ â†’ ê²€ìƒ‰ì–´ ë§¤í•‘ â•â•â•
TICKER_QUERIES = {
    "NVDA": "ì—”ë¹„ë””ì•„", "AVGO": "ë¸Œë¡œë“œì»´", "ASML": "ASML",
    "AMD": "AMD", "QCOM": "í€„ì»´", "TXN": "í…ì‚¬ìŠ¤ì¸ìŠ¤íŠ¸ë£¨ë¨¼íŠ¸",
    "ARM": "ARM ë°˜ë„ì²´", "AMAT": "ì–´í”Œë¼ì´ë“œë¨¸í‹°ë¦¬ì–¼ì¦ˆ",
    "INTC": "ì¸í…” ë°˜ë„ì²´", "ADI": "ì•„ë‚ ë¡œê·¸ë””ë°”ì´ì‹œìŠ¤",
    "MU": "ë§ˆì´í¬ë¡ ", "LRCX": "ë¨ë¦¬ì„œì¹˜", "KLAC": "KLA",
    "MRVL": "ë§ˆë²¨í…Œí¬ë†€ë¡œì§€", "NXPI": "NXPë°˜ë„ì²´", "MCHP": "ë§ˆì´í¬ë¡œì¹©",
    "MPWR": "ëª¨ë†€ë¦¬ì‹íŒŒì›Œ", "STX": "ì‹œê²Œì´íŠ¸", "WDC": "ì›¨ìŠ¤í„´ë””ì§€í„¸",
    "MSFT": "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸", "CSCO": "ì‹œìŠ¤ì½”", "PLTR": "íŒ”ë€í‹°ì–´",
    "CDNS": "ì¼€ì´ë˜ìŠ¤", "SNPS": "ì‹œë†‰ì‹œìŠ¤", "ADBE": "ì–´ë„ë¹„",
    "INTU": "ì¸íŠœì´íŠ¸", "ADP": "ADP", "WDAY": "ì›Œí¬ë°ì´",
    "DDOG": "ë°ì´í„°ë…", "VRSK": "ë²„ë¦¬ìŠ¤í¬", "CTSH": "ì½”ê·¸ë‹ˆì „íŠ¸",
    "CSGP": "ì½”ìŠ¤íƒ€ê·¸ë£¹", "PAYX": "í˜ì´ì²µìŠ¤", "MSTR": "ë§ˆì´í¬ë¡œìŠ¤íŠ¸ë˜í‹°ì§€",
    "PANW": "íŒ”ë¡œì•Œí† ë„¤íŠ¸ì›ìŠ¤", "CRWD": "í¬ë¼ìš°ë“œìŠ¤íŠ¸ë¼ì´í¬",
    "FTNT": "í¬í‹°ë„·", "ZS": "ì§€ìŠ¤ì¼€ì¼ëŸ¬", "TEAM": "ì•„í‹€ë¼ì‹œì•ˆ",
    "ADSK": "ì˜¤í† ë°ìŠ¤í¬", "SHOP": "ì‡¼í”¼íŒŒì´",
    "ROP": "ë¡œí¼í…Œí¬ë†€ë¡œì§€ìŠ¤", "TRI": "í†°ìŠ¨ë¡œì´í„°",
    "GOOGL": "êµ¬ê¸€ ì•ŒíŒŒë²³", "META": "ë©”íƒ€ í˜ì´ìŠ¤ë¶",
    "NFLX": "ë„·í”Œë¦­ìŠ¤", "APP": "ì•±ëŸ¬ë¹ˆ", "DASH": "ë„ì–´ëŒ€ì‹œ",
    "EA": "ì¼ë ‰íŠ¸ë¡œë‹‰ì•„ì¸ ", "TTWO": "í…Œì´í¬íˆ¬",
    "PDD": "í•€ë‘¬ë‘¬ í…Œë¬´", "WBD": "ì›Œë„ˆë¸Œë¼ë”ìŠ¤",
    "CHTR": "ì°¨í„°ì»¤ë®¤ë‹ˆì¼€ì´ì…˜", "CMCSA": "ì»´ìºìŠ¤íŠ¸",
    "AMZN": "ì•„ë§ˆì¡´", "BKNG": "ë¶€í‚¹í™€ë”©ìŠ¤", "MELI": "ë©”ë¥´ì¹´ë„ë¦¬ë¸Œë ˆ",
    "ABNB": "ì—ì–´ë¹„ì•¤ë¹„", "PYPL": "í˜ì´íŒ”", "MAR": "ë©”ë¦¬ì–´íŠ¸",
    "ROST": "ë¡œìŠ¤ìŠ¤í† ì–´ìŠ¤", "WMT": "ì›”ë§ˆíŠ¸",
    "AAPL": "ì• í”Œ", "COST": "ì½”ìŠ¤íŠ¸ì½”", "PEP": "í©ì‹œì½”",
    "TMUS": "Tëª¨ë°”ì¼", "SBUX": "ìŠ¤íƒ€ë²…ìŠ¤", "MDLZ": "ëª¬ë¸ë¦¬ì¦ˆ",
    "MNST": "ëª¬ìŠ¤í„°ë¹„ë²„ë¦¬ì§€", "KHC": "í¬ë˜í”„íŠ¸í•˜ì¸ì¦ˆ",
    "KDP": "íë¦¬ê·¸ë‹¥í„°í˜í¼", "CCEP": "ì½”ì¹´ì½œë¼ìœ ë¡œíŒ¨ì‹œí”½",
    "CEG": "ì»¨ìŠ¤í…”ë ˆì´ì…˜ì—ë„ˆì§€", "XEL": "ì—‘ì…€ì—ë„ˆì§€",
    "AEP": "ì•„ë©”ë¦¬ì¹¸ì¼ë ‰íŠ¸ë¦­íŒŒì›Œ", "EXC": "ì—‘ì…€ë¡ ",
    "ISRG": "ì¸íŠœì´í‹°ë¸Œì„œì§€ì»¬", "AMGN": "ì•”ì  ", "VRTX": "ë²„í…ìŠ¤ì œì•½",
    "GILD": "ê¸¸ë¦¬ì–´ë“œ", "REGN": "ë¦¬ì œë„¤ë¡ ", "GEHC": "GEí—¬ìŠ¤ì¼€ì–´",
    "DXCM": "ë±ìŠ¤ì½¤", "IDXX": "ì•„ì´ë±ìŠ¤", "ALNY": "ì•Œë‚˜ì¼ëŒ",
    "INSM": "ì¸ìŠ¤ë©”ë“œ", "LIN": "ë¦°ë°",
    "TSLA": "í…ŒìŠ¬ë¼", "HON": "í•˜ë‹ˆì›°", "AXON": "ì•¡ìŠ¨ì—”í„°í”„ë¼ì´ì¦ˆ",
    "CSX": "CSX", "CPRT": "ì½”íŒŒíŠ¸", "ODFL": "ì˜¬ë“œë„ë¯¸ë‹ˆì–¸",
    "FAST": "íŒŒìŠ¤ë„", "FANG": "ë‹¤ì´ì•„ëª¬ë“œë°±ì—ë„ˆì§€",
    "BKR": "ë² ì´ì»¤íœ´ì¦ˆ", "FER": "í˜ë¡œë¹„ì•Œ", "PCAR": "íŒ©ì¹´",
    "ORLY": "ì˜¤ë¼ì¼ë¦¬ì˜¤í† ", "CTAS": "ì‹ íƒ€ìŠ¤",
}


def search_naver_news(query, display=5):
    enc = urllib.parse.quote(query)
    url = f"https://openapi.naver.com/v1/search/news.json?query={enc}&display={display}&sort=date"
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", CLIENT_ID)
    req.add_header("X-Naver-Client-Secret", CLIENT_SECRET)
    try:
        with urllib.request.urlopen(req, timeout=10) as resp:
            if resp.getcode() == 200:
                return json.loads(resp.read().decode("utf-8"))
    except Exception as e:
        print(f"  âŒ Error searching '{query}': {e}")
    return None


def clean_html(text):
    text = re.sub(r"<[^>]+>", "", text)
    text = text.replace("&quot;", '"').replace("&amp;", "&")
    text = text.replace("&lt;", "<").replace("&gt;", ">")
    text = text.replace("&apos;", "'")
    return text.strip()


def parse_date(date_str):
    if not date_str:
        return None
    try:
        dt = parsedate_to_datetime(date_str)
        return dt.isoformat()
    except:
        pass
    if "T" in date_str:
        return date_str
    return date_str


def is_within_retention(date_str, cutoff_date):
    if not date_str:
        return True
    try:
        if "T" in date_str:
            dt_str = date_str.split("T")[0]
            dt = datetime.strptime(dt_str, "%Y-%m-%d")
        else:
            dt = parsedate_to_datetime(date_str).replace(tzinfo=None)
        return dt >= cutoff_date
    except:
        return True


def extract_mentioned_tickers(title, desc):
    combined = (title + " " + desc).upper()
    mentions = set()
    for ticker in TICKER_QUERIES:
        if len(ticker) >= 3 and ticker in combined:
            mentions.add(ticker)
    combined_kr = title + " " + desc
    kr_to_ticker = {}
    for t, q in TICKER_QUERIES.items():
        for keyword in q.split():
            if len(keyword) >= 2:
                kr_to_ticker[keyword] = t
    for keyword, ticker in kr_to_ticker.items():
        if keyword in combined_kr:
            mentions.add(ticker)
    return list(mentions)


def load_existing_news(path):
    if not os.path.exists(path):
        print("  ğŸ“„ ê¸°ì¡´ news.json ì—†ìŒ â€” ìƒˆë¡œ ìƒì„±")
        return None
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        article_count = sum(len(v) for v in data.get("stocks", {}).values())
        print(f"  ğŸ“„ ê¸°ì¡´ news.json ë¡œë“œ: {article_count}ê°œ ê¸°ì‚¬")
        return data
    except Exception as e:
        print(f"  âš ï¸ ê¸°ì¡´ news.json ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


def deduplicate_articles(articles):
    """ê¸°ì‚¬ ëª©ë¡ì—ì„œ ì¤‘ë³µ ì œê±° (URL + ì œëª©)"""
    seen_urls = set()
    seen_titles = set()
    unique = []

    def clean_url(u):
        return u.split("?")[0].split("#")[0].rstrip("/") if u else ""

    for art in articles:
        url = clean_url(art.get("url", ""))
        norm_title = normalize_title(art.get("title", ""))[:20]

        # URL ì¤‘ë³µ
        if url and url in seen_urls:
            continue
        # ì œëª© ì• 20ì ì¤‘ë³µ
        if len(norm_title) >= 15 and norm_title in seen_titles:
            continue

        if url:
            seen_urls.add(url)
        if len(norm_title) >= 15:
            seen_titles.add(norm_title)
        unique.append(art)

    return unique


def calculate_co_mentions(stocks_data):
    co_mention_count = {}
    for ticker, articles in stocks_data.items():
        for art in articles:
            tickers_in_article = set(art.get("mentions", []))
            tickers_in_article.add(ticker)
            tickers_list = sorted(tickers_in_article)
            for a_idx in range(len(tickers_list)):
                for b_idx in range(a_idx + 1, len(tickers_list)):
                    pair = f"{tickers_list[a_idx]}-{tickers_list[b_idx]}"
                    co_mention_count[pair] = co_mention_count.get(pair, 0) + 1
    return {
        k: v for k, v in sorted(co_mention_count.items(), key=lambda x: -x[1])
        if v >= 2
    }


def main():
    kst = timezone(timedelta(hours=9))
    now = datetime.now(kst)
    cutoff_date = (now - timedelta(days=RETENTION_DAYS)).replace(tzinfo=None)

    print(f"ğŸš€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {now.strftime('%Y-%m-%d %H:%M KST')}")
    print(f"   ì´ {len(TICKER_QUERIES)}ê°œ ì¢…ëª©")
    print(f"   ë³´ê´€ ê¸°ê°„: {RETENTION_DAYS}ì¼ (~ {cutoff_date.strftime('%Y-%m-%d')} ì´í›„)")

    # â•â•â• 1. ê¸°ì¡´ ë°ì´í„° ë¡œë“œ â•â•â•
    out_path = os.path.join(os.path.dirname(__file__), "..", "data", "news.json")
    out_path = os.path.abspath(out_path)
    existing = load_existing_news(out_path)
    existing_stocks = existing.get("stocks", {}) if existing else {}

    # â•â•â• 2. ê¸°ì¡´ ë°ì´í„°ì—ì„œ ë¨¼ì € ì¤‘ë³µ ì œê±° + ë¹„ê²½ì œ ë‰´ìŠ¤ ì •ë¦¬ â•â•â•
    print(f"\nğŸ§¹ ê¸°ì¡´ ë°ì´í„° ì •ë¦¬ ì¤‘...")
    cleaned_count = 0
    for ticker in list(existing_stocks.keys()):
        before = len(existing_stocks[ticker])
        # ë¹„ê²½ì œ ë‰´ìŠ¤ ì œê±°
        existing_stocks[ticker] = [
            a for a in existing_stocks[ticker]
            if is_financial_news(a.get("title", ""), a.get("desc", ""))
        ]
        # ì¤‘ë³µ ì œê±°
        existing_stocks[ticker] = deduplicate_articles(existing_stocks[ticker])
        # 90ì¼ ì´ˆê³¼ ì œê±°
        existing_stocks[ticker] = [
            a for a in existing_stocks[ticker]
            if is_within_retention(a.get("date"), cutoff_date)
        ]
        cleaned_count += before - len(existing_stocks[ticker])
    if cleaned_count > 0:
        print(f"  ğŸ—‘ï¸ {cleaned_count}ê°œ ê¸°ì‚¬ ì •ë¦¬ë¨ (ì¤‘ë³µ/ë¹„ê²½ì œ/ë§Œë£Œ)")

    # â•â•â• 3. ì˜¤ëŠ˜ ë‰´ìŠ¤ ìˆ˜ì§‘ â•â•â•
    print(f"\nğŸ“¡ ì˜¤ëŠ˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    today_new_count = 0
    today_filtered_count = 0

    for i, (ticker, query) in enumerate(TICKER_QUERIES.items()):
        if (i + 1) % 10 == 0 or i == 0:
            print(f"  [{i+1:3d}/{len(TICKER_QUERIES)}] {ticker}: '{query}'")

        new_articles = []
        seen_urls = set()

        for q in [query, f"{ticker} ì£¼ê°€"]:
            result = search_naver_news(q, display=20)
            if result and "items" in result:
                for item in result["items"]:
                    # originallinkë§Œ ì‚¬ìš© (ë„¤ì´ë²„ ë§í¬ ì°¨ë‹¨)
                    url = item.get("originallink", "")
                    if not url or url in seen_urls:
                        continue
                    if not is_allowed_source(url):
                        continue
                    seen_urls.add(url)

                    title = clean_html(item.get("title", ""))
                    desc = clean_html(item.get("description", ""))

                    # ë¹„ê²½ì œ ë‰´ìŠ¤ í•„í„°
                    if not is_financial_news(title, desc):
                        today_filtered_count += 1
                        continue

                    # ê¸°ì¡´ ê¸°ì‚¬ì™€ ì¤‘ë³µ ì²´í¬
                    if is_duplicate(title, url, existing_stocks.get(ticker, [])):
                        continue

                    mentioned = extract_mentioned_tickers(title, desc)
                    pub_date = parse_date(item.get("pubDate", ""))

                    new_articles.append({
                        "title": title,
                        "desc": desc[:200],
                        "url": url,
                        "date": pub_date,
                        "mentions": mentioned,
                    })

                    if len(new_articles) >= 10:
                        break

            if len(new_articles) >= 10:
                break
            time.sleep(0.05)

        today_new_count += len(new_articles)

        # ê¸°ì¡´ ê¸°ì‚¬ ì•ì— ìƒˆ ê¸°ì‚¬ ì¶”ê°€ (ìµœì‹  ë¨¼ì €)
        combined = new_articles + existing_stocks.get(ticker, [])
        existing_stocks[ticker] = deduplicate_articles(combined)

    # â•â•â• 4. co-mention ì „ì²´ ì¬ê³„ì‚° â•â•â•
    print(f"\nğŸ”— co-mention ì¬ê³„ì‚° ì¤‘...")
    co_mentions = calculate_co_mentions(existing_stocks)

    # â•â•â• 5. í†µê³„ â•â•â•
    total_articles = sum(len(v) for v in existing_stocks.values())
    tickers_with_news = sum(1 for v in existing_stocks.values() if len(v) > 0)

    # â•â•â• 6. ì €ì¥ â•â•â•
    news_data = {
        "updated": now.isoformat(),
        "updated_kst": now.strftime("%Y-%m-%d %H:%M"),
        "retention_days": RETENTION_DAYS,
        "stats": {
            "total_articles": total_articles,
            "tickers_with_news": tickers_with_news,
            "today_new": today_new_count,
            "today_filtered": today_filtered_count,
            "co_mention_pairs": len(co_mentions),
        },
        "stocks": existing_stocks,
        "co_mentions": co_mentions,
    }

    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(news_data, f, ensure_ascii=False, indent=1)

    file_size = os.path.getsize(out_path) / 1024
    print(f"\nâœ… ì™„ë£Œ!")
    print(f"   ì˜¤ëŠ˜ ìˆ˜ì§‘: {today_new_count}ê°œ (í•„í„°ë§ ì œì™¸: {today_filtered_count}ê°œ)")
    print(f"   ì „ì²´ ëˆ„ì : {total_articles}ê°œ ({tickers_with_news}ê°œ ì¢…ëª©)")
    print(f"   co-mention ìŒ: {len(co_mentions)}ê°œ")
    print(f"   íŒŒì¼ í¬ê¸°: {file_size:.1f} KB")

    top = list(co_mentions.items())[:15]
    if top:
        print(f"\nğŸ“Š co-mention TOP 15:")
        for pair, count in top:
            print(f"   {pair}: {count}ê±´")


if __name__ == "__main__":
    main()


if __name__ == "__main__":
    main()
