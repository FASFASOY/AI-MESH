#!/usr/bin/env python3
"""
NASDAQ 100 ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° â€” ë„¤ì´ë²„ ê²€ìƒ‰ API
ë§¤ì¼ GitHub Actionsì—ì„œ ì‹¤í–‰, ê²°ê³¼ë¥¼ data/news.jsonìœ¼ë¡œ ëˆ„ì  ì €ì¥

í•µì‹¬ ë³€ê²½:
- ë§¤ì¼ ë¦¬ì…‹ â†’ 90ì¼(3ê°œì›”) ëˆ„ì  ë°©ì‹
- ê¸°ì¡´ news.json ë¡œë“œ â†’ ìƒˆ ë‰´ìŠ¤ ì¶”ê°€ â†’ 90ì¼ ì´ˆê³¼ ê¸°ì‚¬ ì‚­ì œ
- co-mentionì€ ì „ì²´ ëˆ„ì  ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ë§¤ë²ˆ ì¬ê³„ì‚°
- ì¤‘ë³µ URL ìë™ ì œê±°
"""

import os, json, time, urllib.request, urllib.parse, re
from datetime import datetime, timezone, timedelta
from email.utils import parsedate_to_datetime

CLIENT_ID = os.environ["NAVER_CLIENT_ID"]
CLIENT_SECRET = os.environ["NAVER_CLIENT_SECRET"]

RETENTION_DAYS = 90  # ë‰´ìŠ¤ ë³´ê´€ ê¸°ê°„ (3ê°œì›”)

# â•â•â• í—ˆìš© ì–¸ë¡ ì‚¬ ë„ë©”ì¸ (ê²½ì œ/ê¸ˆìœµ ì „ë¬¸ì§€ë§Œ) â•â•â•
ALLOWED_DOMAINS = [
    "mk.co.kr",           # ë§¤ì¼ê²½ì œ
    "heraldcorp.com",      # í—¤ëŸ´ë“œê²½ì œ
    "herald.co.kr",        # í—¤ëŸ´ë“œê²½ì œ (êµ¬ë„ë©”ì¸)
    "fnnews.com",          # íŒŒì´ë‚¸ì…œë‰´ìŠ¤
    "mt.co.kr",            # ë¨¸ë‹ˆíˆ¬ë°ì´
    "moneytoday.co.kr",    # ë¨¸ë‹ˆíˆ¬ë°ì´ (êµ¬ë„ë©”ì¸)
    "bizwatch.co.kr",      # ë¹„ì¦ˆì›Œì¹˜
    "asiae.co.kr",         # ì•„ì‹œì•„ê²½ì œ
    "edaily.co.kr",        # ì´ë°ì¼ë¦¬
    "biz.chosun.com",      # ì¡°ì„ ë¹„ì¦ˆ
    "hankyung.com",        # í•œêµ­ê²½ì œ
    "joseilbo.com",        # ì¡°ì„¸ì¼ë³´
    "sedaily.com",         # ì„œìš¸ê²½ì œ
]

def is_allowed_source(url):
    if not url:
        return False
    url_lower = url.lower()
    return any(domain in url_lower for domain in ALLOWED_DOMAINS)

# â•â•â• í‹°ì»¤ â†’ ê²€ìƒ‰ì–´ ë§¤í•‘ â•â•â•
TICKER_QUERIES = {
    # Semiconductor
    "NVDA": "ì—”ë¹„ë””ì•„", "AVGO": "ë¸Œë¡œë“œì»´", "ASML": "ASML",
    "AMD": "AMD", "QCOM": "í€„ì»´", "TXN": "í…ì‚¬ìŠ¤ì¸ìŠ¤íŠ¸ë£¨ë¨¼íŠ¸",
    "ARM": "ARM ë°˜ë„ì²´", "AMAT": "ì–´í”Œë¼ì´ë“œë¨¸í‹°ë¦¬ì–¼ì¦ˆ",
    "INTC": "ì¸í…” ë°˜ë„ì²´", "ADI": "ì•„ë‚ ë¡œê·¸ë””ë°”ì´ì‹œìŠ¤",
    "MU": "ë§ˆì´í¬ë¡ ", "LRCX": "ë¨ë¦¬ì„œì¹˜", "KLAC": "KLA",
    "MRVL": "ë§ˆë²¨í…Œí¬ë†€ë¡œì§€", "NXPI": "NXPë°˜ë„ì²´", "MCHP": "ë§ˆì´í¬ë¡œì¹©",
    "MPWR": "ëª¨ë†€ë¦¬ì‹íŒŒì›Œ", "STX": "ì‹œê²Œì´íŠ¸", "WDC": "ì›¨ìŠ¤í„´ë””ì§€í„¸",
    # Software & Cloud
    "MSFT": "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸", "CSCO": "ì‹œìŠ¤ì½”", "PLTR": "íŒ”ë€í‹°ì–´",
    "CDNS": "ì¼€ì´ë˜ìŠ¤", "SNPS": "ì‹œë†‰ì‹œìŠ¤", "ADBE": "ì–´ë„ë¹„",
    "INTU": "ì¸íŠœì´íŠ¸", "ADP": "ADP", "WDAY": "ì›Œí¬ë°ì´",
    "DDOG": "ë°ì´í„°ë…", "VRSK": "ë²„ë¦¬ìŠ¤í¬", "CTSH": "ì½”ê·¸ë‹ˆì „íŠ¸",
    "CSGP": "ì½”ìŠ¤íƒ€ê·¸ë£¹", "PAYX": "í˜ì´ì²µìŠ¤", "MSTR": "ë§ˆì´í¬ë¡œìŠ¤íŠ¸ë˜í‹°ì§€",
    "PANW": "íŒ”ë¡œì•Œí† ë„¤íŠ¸ì›ìŠ¤", "CRWD": "í¬ë¼ìš°ë“œìŠ¤íŠ¸ë¼ì´í¬",
    "FTNT": "í¬í‹°ë„·", "ZS": "ì§€ìŠ¤ì¼€ì¼ëŸ¬", "TEAM": "ì•„í‹€ë¼ì‹œì•ˆ",
    "ADSK": "ì˜¤í† ë°ìŠ¤í¬", "SHOP": "ì‡¼í”¼íŒŒì´",
    "ROP": "ë¡œí¼í…Œí¬ë†€ë¡œì§€ìŠ¤", "TRI": "í†°ìŠ¨ë¡œì´í„°",
    # Internet & Info
    "GOOGL": "êµ¬ê¸€ ì•ŒíŒŒë²³", "META": "ë©”íƒ€ í˜ì´ìŠ¤ë¶",
    "NFLX": "ë„·í”Œë¦­ìŠ¤", "APP": "ì•±ëŸ¬ë¹ˆ", "DASH": "ë„ì–´ëŒ€ì‹œ",
    "EA": "ì¼ë ‰íŠ¸ë¡œë‹‰ì•„ì¸ ", "TTWO": "í…Œì´í¬íˆ¬",
    "PDD": "í•€ë‘¬ë‘¬ í…Œë¬´", "WBD": "ì›Œë„ˆë¸Œë¼ë”ìŠ¤",
    "CHTR": "ì°¨í„°ì»¤ë®¤ë‹ˆì¼€ì´ì…˜", "CMCSA": "ì»´ìºìŠ¤íŠ¸",
    # Internet Retail
    "AMZN": "ì•„ë§ˆì¡´", "BKNG": "ë¶€í‚¹í™€ë”©ìŠ¤", "MELI": "ë©”ë¥´ì¹´ë„ë¦¬ë¸Œë ˆ",
    "ABNB": "ì—ì–´ë¹„ì•¤ë¹„", "PYPL": "í˜ì´íŒ”", "MAR": "ë©”ë¦¬ì–´íŠ¸",
    "ROST": "ë¡œìŠ¤ìŠ¤í† ì–´ìŠ¤", "WMT": "ì›”ë§ˆíŠ¸",
    # Consumer Lifestyle
    "AAPL": "ì• í”Œ", "COST": "ì½”ìŠ¤íŠ¸ì½”", "PEP": "í©ì‹œì½”",
    "TMUS": "Tëª¨ë°”ì¼", "SBUX": "ìŠ¤íƒ€ë²…ìŠ¤", "MDLZ": "ëª¬ë¸ë¦¬ì¦ˆ",
    "MNST": "ëª¬ìŠ¤í„°ë¹„ë²„ë¦¬ì§€", "KHC": "í¬ë˜í”„íŠ¸í•˜ì¸ì¦ˆ",
    "KDP": "íë¦¬ê·¸ë‹¥í„°í˜í¼", "CCEP": "ì½”ì¹´ì½œë¼ìœ ë¡œíŒ¨ì‹œí”½",
    "CEG": "ì»¨ìŠ¤í…”ë ˆì´ì…˜ì—ë„ˆì§€", "XEL": "ì—‘ì…€ì—ë„ˆì§€",
    "AEP": "ì•„ë©”ë¦¬ì¹¸ì¼ë ‰íŠ¸ë¦­íŒŒì›Œ", "EXC": "ì—‘ì…€ë¡ ",
    # Healthcare
    "ISRG": "ì¸íŠœì´í‹°ë¸Œì„œì§€ì»¬", "AMGN": "ì•”ì  ", "VRTX": "ë²„í…ìŠ¤ì œì•½",
    "GILD": "ê¸¸ë¦¬ì–´ë“œ", "REGN": "ë¦¬ì œë„¤ë¡ ", "GEHC": "GEí—¬ìŠ¤ì¼€ì–´",
    "DXCM": "ë±ìŠ¤ì½¤", "IDXX": "ì•„ì´ë±ìŠ¤", "ALNY": "ì•Œë‚˜ì¼ëŒ",
    "INSM": "ì¸ìŠ¤ë©”ë“œ", "LIN": "ë¦°ë°",
    # Mobility & Industrial
    "TSLA": "í…ŒìŠ¬ë¼", "HON": "í•˜ë‹ˆì›°", "AXON": "ì•¡ìŠ¨ì—”í„°í”„ë¼ì´ì¦ˆ",
    "CSX": "CSX", "CPRT": "ì½”íŒŒíŠ¸", "ODFL": "ì˜¬ë“œë„ë¯¸ë‹ˆì–¸",
    "FAST": "íŒŒìŠ¤ë„", "FANG": "ë‹¤ì´ì•„ëª¬ë“œë°±ì—ë„ˆì§€",
    "BKR": "ë² ì´ì»¤íœ´ì¦ˆ", "FER": "í˜ë¡œë¹„ì•Œ", "PCAR": "íŒ©ì¹´",
    "ORLY": "ì˜¤ë¼ì¼ë¦¬ì˜¤í† ", "CTAS": "ì‹ íƒ€ìŠ¤",
}


def search_naver_news(query, display=5):
    enc = urllib.parse.quote(query)
    url = f"https://openapi.naver.com/v1/search/news.json?query={enc}&display={display}&sort=date"
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", CLIENT_ID)
    req.add_header("X-Naver-Client-Secret", CLIENT_SECRET)
    try:
        with urllib.request.urlopen(req, timeout=10) as resp:
            if resp.getcode() == 200:
                return json.loads(resp.read().decode("utf-8"))
    except Exception as e:
        print(f"  âŒ Error searching '{query}': {e}")
    return None


def clean_html(text):
    text = re.sub(r"<[^>]+>", "", text)
    text = text.replace("&quot;", '"').replace("&amp;", "&")
    text = text.replace("&lt;", "<").replace("&gt;", ">")
    text = text.replace("&apos;", "'")
    return text.strip()


def parse_date(date_str):
    """ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ì„ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    if not date_str:
        return None
    try:
        # RFC 2822 í˜•ì‹ (ë„¤ì´ë²„ API ê¸°ë³¸)
        dt = parsedate_to_datetime(date_str)
        return dt.isoformat()
    except:
        pass
    # ì´ë¯¸ ISO í˜•ì‹ì¸ ê²½ìš°
    if "T" in date_str:
        return date_str
    return date_str


def is_within_retention(date_str, cutoff_date):
    """ê¸°ì‚¬ê°€ ë³´ê´€ ê¸°ê°„ ë‚´ì¸ì§€ í™•ì¸"""
    if not date_str:
        return True  # ë‚ ì§œ ì—†ìœ¼ë©´ ì¼ë‹¨ ë³´ê´€
    try:
        if "T" in date_str:
            # ISO í˜•ì‹
            dt_str = date_str.split("T")[0]
            dt = datetime.strptime(dt_str, "%Y-%m-%d")
        else:
            # RFC 2822
            dt = parsedate_to_datetime(date_str).replace(tzinfo=None)
        return dt >= cutoff_date
    except:
        return True  # íŒŒì‹± ì‹¤íŒ¨í•˜ë©´ ì¼ë‹¨ ë³´ê´€


def extract_mentioned_tickers(title, desc):
    combined = (title + " " + desc).upper()
    mentions = set()
    for ticker in TICKER_QUERIES:
        if len(ticker) >= 3 and ticker in combined:
            mentions.add(ticker)
    combined_kr = title + " " + desc
    kr_to_ticker = {}
    for t, q in TICKER_QUERIES.items():
        for keyword in q.split():
            if len(keyword) >= 2:
                kr_to_ticker[keyword] = t
    for keyword, ticker in kr_to_ticker.items():
        if keyword in combined_kr:
            mentions.add(ticker)
    return list(mentions)


def load_existing_news(path):
    """ê¸°ì¡´ news.json ë¡œë“œ"""
    if not os.path.exists(path):
        print("  ğŸ“„ ê¸°ì¡´ news.json ì—†ìŒ â€” ìƒˆë¡œ ìƒì„±")
        return None
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        stock_count = len(data.get("stocks", {}))
        article_count = sum(len(v) for v in data.get("stocks", {}).values())
        print(f"  ğŸ“„ ê¸°ì¡´ news.json ë¡œë“œ: {stock_count}ê°œ ì¢…ëª©, {article_count}ê°œ ê¸°ì‚¬")
        return data
    except Exception as e:
        print(f"  âš ï¸ ê¸°ì¡´ news.json ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


def merge_articles(existing_articles, new_articles):
    """ê¸°ì¡´ ê¸°ì‚¬ + ìƒˆ ê¸°ì‚¬ ë³‘í•© (URL ê¸°ì¤€ ì¤‘ë³µ ì œê±°)"""
    seen_urls = set()
    merged = []

    # ìƒˆ ê¸°ì‚¬ ë¨¼ì € (ìµœì‹  ìš°ì„ )
    for art in new_articles:
        url = art.get("url", "")
        if url and url not in seen_urls:
            seen_urls.add(url)
            merged.append(art)

    # ê¸°ì¡´ ê¸°ì‚¬ ì¶”ê°€
    for art in existing_articles:
        url = art.get("url", "")
        if url and url not in seen_urls:
            seen_urls.add(url)
            merged.append(art)

    return merged


def calculate_co_mentions(stocks_data):
    """ì „ì²´ ëˆ„ì  ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ co-mention ì¬ê³„ì‚°"""
    co_mention_count = {}

    for ticker, articles in stocks_data.items():
        for art in articles:
            tickers_in_article = set(art.get("mentions", []))
            tickers_in_article.add(ticker)
            tickers_list = sorted(tickers_in_article)
            for a_idx in range(len(tickers_list)):
                for b_idx in range(a_idx + 1, len(tickers_list)):
                    pair = f"{tickers_list[a_idx]}-{tickers_list[b_idx]}"
                    co_mention_count[pair] = co_mention_count.get(pair, 0) + 1

    # 2íšŒ ì´ìƒë§Œ ì €ì¥, ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    return {
        k: v for k, v in sorted(co_mention_count.items(), key=lambda x: -x[1])
        if v >= 2
    }


def main():
    kst = timezone(timedelta(hours=9))
    now = datetime.now(kst)
    cutoff_date = (now - timedelta(days=RETENTION_DAYS)).replace(tzinfo=None)

    print(f"ğŸš€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {now.strftime('%Y-%m-%d %H:%M KST')}")
    print(f"   ì´ {len(TICKER_QUERIES)}ê°œ ì¢…ëª©")
    print(f"   ë³´ê´€ ê¸°ê°„: {RETENTION_DAYS}ì¼ (~ {cutoff_date.strftime('%Y-%m-%d')} ì´í›„)")

    # â•â•â• 1. ê¸°ì¡´ ë°ì´í„° ë¡œë“œ â•â•â•
    out_path = os.path.join(os.path.dirname(__file__), "..", "data", "news.json")
    out_path = os.path.abspath(out_path)
    existing = load_existing_news(out_path)
    existing_stocks = existing.get("stocks", {}) if existing else {}

    # â•â•â• 2. ì˜¤ëŠ˜ ë‰´ìŠ¤ ìˆ˜ì§‘ â•â•â•
    print(f"\nğŸ“¡ ì˜¤ëŠ˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    today_new_count = 0

    for i, (ticker, query) in enumerate(TICKER_QUERIES.items()):
        if (i + 1) % 10 == 0 or i == 0:
            print(f"  [{i+1:3d}/{len(TICKER_QUERIES)}] {ticker}: '{query}'")

        # ì˜¤ëŠ˜ ìƒˆë¡œ ìˆ˜ì§‘
        new_articles = []
        seen_urls = set()

        for q in [query, f"{ticker} ì£¼ê°€"]:
            result = search_naver_news(q, display=20)
            if result and "items" in result:
                for item in result["items"]:
                    url = item.get("originallink") or item.get("link", "")
                    if url in seen_urls:
                        continue
                    if not is_allowed_source(url):
                        continue
                    seen_urls.add(url)

                    title = clean_html(item.get("title", ""))
                    desc = clean_html(item.get("description", ""))
                    mentioned = extract_mentioned_tickers(title, desc)
                    pub_date = parse_date(item.get("pubDate", ""))

                    new_articles.append({
                        "title": title,
                        "desc": desc[:200],
                        "url": url,
                        "date": pub_date,
                        "mentions": mentioned,
                    })

                    if len(new_articles) >= 10:
                        break

            if len(new_articles) >= 10:
                break
            time.sleep(0.05)

        today_new_count += len(new_articles)

        # â•â•â• 3. ê¸°ì¡´ ê¸°ì‚¬ì™€ ë³‘í•© â•â•â•
        old_articles = existing_stocks.get(ticker, [])
        merged = merge_articles(old_articles, new_articles)

        # â•â•â• 4. 90ì¼ ì´ˆê³¼ ê¸°ì‚¬ ì œê±° â•â•â•
        retained = [a for a in merged if is_within_retention(a.get("date"), cutoff_date)]

        existing_stocks[ticker] = retained

    # â•â•â• 5. co-mention ì „ì²´ ì¬ê³„ì‚° â•â•â•
    print(f"\nğŸ”— co-mention ì¬ê³„ì‚° ì¤‘...")
    co_mentions = calculate_co_mentions(existing_stocks)

    # â•â•â• 6. í†µê³„ â•â•â•
    total_articles = sum(len(v) for v in existing_stocks.values())
    tickers_with_news = sum(1 for v in existing_stocks.values() if len(v) > 0)

    # â•â•â• 7. ì €ì¥ â•â•â•
    news_data = {
        "updated": now.isoformat(),
        "updated_kst": now.strftime("%Y-%m-%d %H:%M"),
        "retention_days": RETENTION_DAYS,
        "stats": {
            "total_articles": total_articles,
            "tickers_with_news": tickers_with_news,
            "today_new": today_new_count,
            "co_mention_pairs": len(co_mentions),
        },
        "stocks": existing_stocks,
        "co_mentions": co_mentions,
    }

    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(news_data, f, ensure_ascii=False, indent=1)

    file_size = os.path.getsize(out_path) / 1024
    print(f"\nâœ… ì™„ë£Œ!")
    print(f"   ì˜¤ëŠ˜ ìˆ˜ì§‘: {today_new_count}ê°œ")
    print(f"   ì „ì²´ ëˆ„ì : {total_articles}ê°œ ({tickers_with_news}ê°œ ì¢…ëª©)")
    print(f"   co-mention ìŒ: {len(co_mentions)}ê°œ")
    print(f"   íŒŒì¼ í¬ê¸°: {file_size:.1f} KB")
    print(f"   ì €ì¥: {out_path}")

    # ìƒìœ„ co-mention
    top = list(co_mentions.items())[:15]
    if top:
        print(f"\nğŸ“Š co-mention TOP 15:")
        for pair, count in top:
            print(f"   {pair}: {count}ê±´")


if __name__ == "__main__":
    main()
