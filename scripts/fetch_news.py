#!/usr/bin/env python3
"""
NASDAQ 100 ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° â€” ë„¤ì´ë²„ ê²€ìƒ‰ API
ë§¤ì¼ GitHub Actionsì—ì„œ ì‹¤í–‰, ê²°ê³¼ë¥¼ data/news.jsonìœ¼ë¡œ ì €ì¥
"""

import os, json, time, urllib.request, urllib.parse
from datetime import datetime, timezone, timedelta

CLIENT_ID = os.environ["NAVER_CLIENT_ID"]
CLIENT_SECRET = os.environ["NAVER_CLIENT_SECRET"]

# â•â•â• í‹°ì»¤ â†’ ê²€ìƒ‰ì–´ ë§¤í•‘ (í•œê¸€ + ì˜ë¬¸) â•â•â•
# ë„¤ì´ë²„ì—ì„œ ì˜ ê²€ìƒ‰ë˜ë„ë¡ í•œê¸€ ê¸°ì—…ëª… ìš°ì„ , ì˜ë¬¸ í‹°ì»¤ ë³´ì¡°
TICKER_QUERIES = {
    # Semiconductor
    "NVDA": "ì—”ë¹„ë””ì•„", "AVGO": "ë¸Œë¡œë“œì»´", "ASML": "ASML",
    "AMD": "AMD", "QCOM": "í€„ì»´", "TXN": "í…ì‚¬ìŠ¤ì¸ìŠ¤íŠ¸ë£¨ë¨¼íŠ¸",
    "ARM": "ARM ë°˜ë„ì²´", "AMAT": "ì–´í”Œë¼ì´ë“œë¨¸í‹°ë¦¬ì–¼ì¦ˆ",
    "INTC": "ì¸í…” ë°˜ë„ì²´", "ADI": "ì•„ë‚ ë¡œê·¸ë””ë°”ì´ì‹œìŠ¤",
    "MU": "ë§ˆì´í¬ë¡ ", "LRCX": "ë¨ë¦¬ì„œì¹˜", "KLAC": "KLA",
    "MRVL": "ë§ˆë²¨í…Œí¬ë†€ë¡œì§€", "NXPI": "NXPë°˜ë„ì²´", "MCHP": "ë§ˆì´í¬ë¡œì¹©",
    "MPWR": "ëª¨ë†€ë¦¬ì‹íŒŒì›Œ", "STX": "ì‹œê²Œì´íŠ¸", "WDC": "ì›¨ìŠ¤í„´ë””ì§€í„¸",
    # Software & Cloud
    "MSFT": "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸", "CSCO": "ì‹œìŠ¤ì½”", "PLTR": "íŒ”ë€í‹°ì–´",
    "CDNS": "ì¼€ì´ë˜ìŠ¤", "SNPS": "ì‹œë†‰ì‹œìŠ¤", "ADBE": "ì–´ë„ë¹„",
    "INTU": "ì¸íŠœì´íŠ¸", "ADP": "ADP", "WDAY": "ì›Œí¬ë°ì´",
    "DDOG": "ë°ì´í„°ë…", "VRSK": "ë²„ë¦¬ìŠ¤í¬", "CTSH": "ì½”ê·¸ë‹ˆì „íŠ¸",
    "CSGP": "ì½”ìŠ¤íƒ€ê·¸ë£¹", "PAYX": "í˜ì´ì²µìŠ¤", "MSTR": "ë§ˆì´í¬ë¡œìŠ¤íŠ¸ë˜í‹°ì§€",
    "PANW": "íŒ”ë¡œì•Œí† ë„¤íŠ¸ì›ìŠ¤", "CRWD": "í¬ë¼ìš°ë“œìŠ¤íŠ¸ë¼ì´í¬",
    "FTNT": "í¬í‹°ë„·", "ZS": "ì§€ìŠ¤ì¼€ì¼ëŸ¬", "TEAM": "ì•„í‹€ë¼ì‹œì•ˆ",
    "ADSK": "ì˜¤í† ë°ìŠ¤í¬", "SHOP": "ì‡¼í”¼íŒŒì´",
    "ROP": "ë¡œí¼í…Œí¬ë†€ë¡œì§€ìŠ¤", "TRI": "í†°ìŠ¨ë¡œì´í„°",
    # Internet & Info
    "GOOGL": "êµ¬ê¸€ ì•ŒíŒŒë²³", "META": "ë©”íƒ€ í˜ì´ìŠ¤ë¶",
    "NFLX": "ë„·í”Œë¦­ìŠ¤", "APP": "ì•±ëŸ¬ë¹ˆ", "DASH": "ë„ì–´ëŒ€ì‹œ",
    "EA": "ì¼ë ‰íŠ¸ë¡œë‹‰ì•„ì¸ ", "TTWO": "í…Œì´í¬íˆ¬",
    "PDD": "í•€ë‘¬ë‘¬ í…Œë¬´", "WBD": "ì›Œë„ˆë¸Œë¼ë”ìŠ¤",
    "CHTR": "ì°¨í„°ì»¤ë®¤ë‹ˆì¼€ì´ì…˜", "CMCSA": "ì»´ìºìŠ¤íŠ¸",
    # Internet Retail
    "AMZN": "ì•„ë§ˆì¡´", "BKNG": "ë¶€í‚¹í™€ë”©ìŠ¤", "MELI": "ë©”ë¥´ì¹´ë„ë¦¬ë¸Œë ˆ",
    "ABNB": "ì—ì–´ë¹„ì•¤ë¹„", "PYPL": "í˜ì´íŒ”", "MAR": "ë©”ë¦¬ì–´íŠ¸",
    "ROST": "ë¡œìŠ¤ìŠ¤í† ì–´ìŠ¤", "WMT": "ì›”ë§ˆíŠ¸",
    # Consumer Lifestyle
    "AAPL": "ì• í”Œ", "COST": "ì½”ìŠ¤íŠ¸ì½”", "PEP": "í©ì‹œì½”",
    "TMUS": "Tëª¨ë°”ì¼", "SBUX": "ìŠ¤íƒ€ë²…ìŠ¤", "MDLZ": "ëª¬ë¸ë¦¬ì¦ˆ",
    "MNST": "ëª¬ìŠ¤í„°ë¹„ë²„ë¦¬ì§€", "KHC": "í¬ë˜í”„íŠ¸í•˜ì¸ì¦ˆ",
    "KDP": "íë¦¬ê·¸ë‹¥í„°í˜í¼", "CCEP": "ì½”ì¹´ì½œë¼ìœ ë¡œíŒ¨ì‹œí”½",
    "CEG": "ì»¨ìŠ¤í…”ë ˆì´ì…˜ì—ë„ˆì§€", "XEL": "ì—‘ì…€ì—ë„ˆì§€",
    "AEP": "ì•„ë©”ë¦¬ì¹¸ì¼ë ‰íŠ¸ë¦­íŒŒì›Œ", "EXC": "ì—‘ì…€ë¡ ",
    # Healthcare
    "ISRG": "ì¸íŠœì´í‹°ë¸Œì„œì§€ì»¬", "AMGN": "ì•”ì  ", "VRTX": "ë²„í…ìŠ¤ì œì•½",
    "GILD": "ê¸¸ë¦¬ì–´ë“œ", "REGN": "ë¦¬ì œë„¤ë¡ ", "GEHC": "GEí—¬ìŠ¤ì¼€ì–´",
    "DXCM": "ë±ìŠ¤ì½¤", "IDXX": "ì•„ì´ë±ìŠ¤", "ALNY": "ì•Œë‚˜ì¼ëŒ",
    "INSM": "ì¸ìŠ¤ë©”ë“œ", "LIN": "ë¦°ë°",
    # Mobility & Industrial
    "TSLA": "í…ŒìŠ¬ë¼", "HON": "í•˜ë‹ˆì›°", "AXON": "ì•¡ìŠ¨ì—”í„°í”„ë¼ì´ì¦ˆ",
    "CSX": "CSX", "CPRT": "ì½”íŒŒíŠ¸", "ODFL": "ì˜¬ë“œë„ë¯¸ë‹ˆì–¸",
    "FAST": "íŒŒìŠ¤ë„", "FANG": "ë‹¤ì´ì•„ëª¬ë“œë°±ì—ë„ˆì§€",
    "BKR": "ë² ì´ì»¤íœ´ì¦ˆ", "FER": "í˜ë¡œë¹„ì•Œ", "PCAR": "íŒ©ì¹´",
    "ORLY": "ì˜¤ë¼ì¼ë¦¬ì˜¤í† ", "CTAS": "ì‹ íƒ€ìŠ¤",
}

def search_naver_news(query, display=5):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API í˜¸ì¶œ"""
    enc = urllib.parse.quote(query)
    url = f"https://openapi.naver.com/v1/search/news.json?query={enc}&display={display}&sort=date"
    
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", CLIENT_ID)
    req.add_header("X-Naver-Client-Secret", CLIENT_SECRET)
    
    try:
        with urllib.request.urlopen(req, timeout=10) as resp:
            if resp.getcode() == 200:
                return json.loads(resp.read().decode("utf-8"))
    except Exception as e:
        print(f"  âŒ Error searching '{query}': {e}")
    return None


def clean_html(text):
    """HTML íƒœê·¸ ë° ì—”í‹°í‹° ì œê±°"""
    import re
    text = re.sub(r"<[^>]+>", "", text)
    text = text.replace("&quot;", '"').replace("&amp;", "&")
    text = text.replace("&lt;", "<").replace("&gt;", ">")
    text = text.replace("&apos;", "'")
    return text.strip()


def extract_mentioned_tickers(title, desc):
    """ë‰´ìŠ¤ ì œëª©+ë³¸ë¬¸ì—ì„œ ë‹¤ë¥¸ NASDAQ 100 í‹°ì»¤/ê¸°ì—…ëª… ì–¸ê¸‰ ì¶”ì¶œ"""
    combined = (title + " " + desc).upper()
    mentions = set()
    
    # í‹°ì»¤ ì§ì ‘ ë§¤ì¹­ (ìµœì†Œ ê²½ê³„ ë¬¸ì ì²´í¬)
    for ticker in TICKER_QUERIES:
        if len(ticker) >= 3 and ticker in combined:
            mentions.add(ticker)
    
    # í•œê¸€ ê¸°ì—…ëª… ë§¤ì¹­
    combined_kr = title + " " + desc
    kr_to_ticker = {}
    for t, q in TICKER_QUERIES.items():
        for keyword in q.split():
            if len(keyword) >= 2:
                kr_to_ticker[keyword] = t
    
    for keyword, ticker in kr_to_ticker.items():
        if keyword in combined_kr:
            mentions.add(ticker)
    
    return list(mentions)


def main():
    kst = timezone(timedelta(hours=9))
    now = datetime.now(kst)
    print(f"ğŸš€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {now.strftime('%Y-%m-%d %H:%M KST')}")
    print(f"   ì´ {len(TICKER_QUERIES)}ê°œ ì¢…ëª©")
    
    news_data = {
        "updated": now.isoformat(),
        "updated_kst": now.strftime("%Y-%m-%d %H:%M"),
        "stocks": {},        # ticker â†’ [ë‰´ìŠ¤ ëª©ë¡]
        "co_mentions": {},   # "TICKER1-TICKER2" â†’ count (ë™ì‹œ ì–¸ê¸‰)
    }
    
    co_mention_count = {}
    total_articles = 0
    
    for i, (ticker, query) in enumerate(TICKER_QUERIES.items()):
        print(f"  [{i+1:3d}/{len(TICKER_QUERIES)}] {ticker}: '{query}'")
        
        # í•œê¸€ ê²€ìƒ‰ + ì˜ë¬¸ í‹°ì»¤ ê²€ìƒ‰ (ê²°ê³¼ í•©ì¹˜ê¸°)
        articles = []
        seen_urls = set()
        
        for q in [query, f"{ticker} ì£¼ê°€"]:
            result = search_naver_news(q, display=5)
            if result and "items" in result:
                for item in result["items"]:
                    url = item.get("originallink") or item.get("link", "")
                    if url in seen_urls:
                        continue
                    seen_urls.add(url)
                    
                    title = clean_html(item.get("title", ""))
                    desc = clean_html(item.get("description", ""))
                    
                    # ë‹¤ë¥¸ ì¢…ëª© ë™ì‹œ ì–¸ê¸‰ ì¶”ì¶œ
                    mentioned = extract_mentioned_tickers(title, desc)
                    
                    articles.append({
                        "title": title,
                        "desc": desc[:200],
                        "url": url,
                        "date": item.get("pubDate", ""),
                        "mentions": mentioned,
                    })
            
            time.sleep(0.05)  # ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€
        
        # ìµœì‹ ìˆœ ì •ë ¬, ìµœëŒ€ 5ê°œ
        articles = articles[:5]
        news_data["stocks"][ticker] = articles
        total_articles += len(articles)
        
        # ë™ì‹œ ì–¸ê¸‰ ì¹´ìš´íŠ¸
        for art in articles:
            tickers_in_article = set(art["mentions"])
            tickers_in_article.add(ticker)
            tickers_list = sorted(tickers_in_article)
            for a_idx in range(len(tickers_list)):
                for b_idx in range(a_idx + 1, len(tickers_list)):
                    pair = f"{tickers_list[a_idx]}-{tickers_list[b_idx]}"
                    co_mention_count[pair] = co_mention_count.get(pair, 0) + 1
    
    # ë™ì‹œ ì–¸ê¸‰ 2íšŒ ì´ìƒë§Œ ì €ì¥
    news_data["co_mentions"] = {
        k: v for k, v in sorted(co_mention_count.items(), key=lambda x: -x[1])
        if v >= 2
    }
    
    # ì €ì¥
    out_path = os.path.join(os.path.dirname(__file__), "..", "data", "news.json")
    out_path = os.path.abspath(out_path)
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(news_data, f, ensure_ascii=False, indent=1)
    
    print(f"\nâœ… ì™„ë£Œ!")
    print(f"   ê¸°ì‚¬ ìˆ˜: {total_articles}")
    print(f"   ë™ì‹œ ì–¸ê¸‰ ìŒ: {len(news_data['co_mentions'])}")
    print(f"   ì €ì¥: {out_path}")
    
    # ìƒìœ„ ë™ì‹œ ì–¸ê¸‰ ì¶œë ¥
    top = list(news_data["co_mentions"].items())[:15]
    if top:
        print(f"\nğŸ“Š ë™ì‹œ ì–¸ê¸‰ TOP 15:")
        for pair, count in top:
            print(f"   {pair}: {count}ê±´")


if __name__ == "__main__":
    main()
